{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML for currency prediction: USD/CHF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm \n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#%cd \"/content/drive/My Drive/ml-project-2-marmlla_2/\"\n",
    "import Helper.Preprocessing as pp \n",
    "import Helper.Model as ml \n",
    "import Helper.Display as ds\n",
    "import Helper.Pipeline as pl\n",
    "\n",
    "df = pd.read_csv(\"Data/data_daily/dataset_daily.csv\")\n",
    "data = []\n",
    "for i in df.columns[1:]:\n",
    "    data.append(df[i])\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_price_df = pd.DataFrame(index=['MSE','ACC','MEAN RET %'])\n",
    "result_trend_df = pd.DataFrame(index=['ACC','MEAN RET %'])\n",
    "result_price_df_std = pd.DataFrame(index=['MSE','ACC','MEAN RET %'])\n",
    "result_trend_df_std = pd.DataFrame(index=['ACC','MEAN RET %'])\n",
    "\n",
    "device = 'cpu'\n",
    "LOOKBACK = 6\n",
    "span_trend = 1\n",
    "fraction_val = 0.125\n",
    "fraction_test = 0.2    \n",
    "\n",
    "# train_y, val_y, test_y WITH TREND LABEL: 0 or 1.\n",
    "train_x, val_x, test_x, train_y, val_y, test_y, _, price_tuple= pp.generate_dataset(\n",
    "  data,\n",
    "  lookback_=LOOKBACK,\n",
    "  trend_=True,\n",
    "  span_trend_=span_trend,\n",
    "  span_back_trend_= 0,\n",
    "  norm_=True,\n",
    "  fraction_val_=fraction_val,\n",
    "  fraction_test_=fraction_test)\n",
    "\n",
    "# rain_y_reg, val_y_reg, test_y_reg WITH PRICE LABEL: USD/CHF.\n",
    "_, _, _, train_y_reg, val_y_reg, test_y_reg, norm, _= pp.generate_dataset(\n",
    "  data,\n",
    "  lookback_=LOOKBACK,\n",
    "  trend_=False,\n",
    "  span_trend_=span_trend,\n",
    "  norm_=True,\n",
    "  fraction_val_=fraction_val,\n",
    "  fraction_test_=fraction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[np.isnan(train_x)] = 0.5 \n",
    "val_x[np.isnan(val_x)] = 0.5\n",
    "test_x[np.isnan(test_x)] = 0.5\n",
    "train_y_reg[np.isnan(train_y_reg)] = 0.5 \n",
    "val_y_reg[np.isnan(val_y_reg)] = 0.5\n",
    "test_y_reg[np.isnan(test_y_reg)] = 0.5\n",
    "\n",
    "train_y_reg[np.isnan(train_y_reg)] , val_y_reg[np.isnan(val_y_reg)], test_y_reg[np.isnan(test_y_reg)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = train_y[:,0]\n",
    "positive_weight = torch.tensor(\n",
    "        len(buff[buff == 0.0]) / len(buff[buff == 1.0])\n",
    "    ).float().to(device)\n",
    "print(positive_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = len(train_x)\n",
    "L2 = len(val_x) + L1\n",
    "L3 = len(test_x) + L2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "nl = 2\n",
    "n_n = 32\n",
    "tax_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Apply Auto-Encoder to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AutoEncoder = ml.Auto_Encoder(\n",
    "    input_size=train_x.shape[2],\n",
    "    nb_channel_conv=8\n",
    "    )\n",
    "model_AutoEncoder.load_state_dict(torch.load('Helper/model/model_AutoEncoder.pth'))\n",
    "model_AutoEncoder.to(device)\n",
    "model_AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AutoEncoder.eval()\n",
    "inp1 = torch.from_numpy(np.swapaxes(train_x,1,2))\n",
    "out = model_AutoEncoder(inp1.to(device).float())\n",
    "train_x_smooth = np.swapaxes(out.cpu().detach().numpy(),2,1)\n",
    "\n",
    "inp2 = torch.from_numpy(np.swapaxes(val_x,1,2))\n",
    "out = model_AutoEncoder(inp2.to(device).float())\n",
    "val_x_smooth = np.swapaxes(out.cpu().detach().numpy(),2,1)\n",
    "\n",
    "inp3 = torch.from_numpy(np.swapaxes(test_x,1,2))\n",
    "out = model_AutoEncoder(inp3.to(device).float())\n",
    "test_x_smooth = np.swapaxes(out.cpu().detach().numpy(),2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Price prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save augmented datasets with all 17 + 17 features for future use\n",
    "# RUN THE TWO CELLS IN ORDER\n",
    "train_x_aug = train_x.copy()\n",
    "test_x_aug = test_x.copy()\n",
    "val_x_aug = val_x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice to extract original features\n",
    "train_x = train_x_aug[:,:,::2]\n",
    "test_x = test_x_aug[:,:,::2]\n",
    "val_x = val_x_aug[:,:,::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_x_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = nn,\n",
    "        input_size = train_x.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense.to(device)\n",
    "    model_Dense\n",
    "\n",
    "    lr= 0.001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense,\n",
    "        train_x_ = train_x.reshape(train_x.shape[0],-1),\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x.reshape(val_x.shape[0],-1),\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_Dense, targets, MSE = pl.evaluate(model_Dense, test_x.reshape(test_x.shape[0],-1), test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_Dense = pp.min_max_norm_inverse(outputs_Dense.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_Dense = np.mean((outputs_Dense-t)**2)\n",
    "    ACC_Dense = pl.direction_accuracy(outputs_Dense, t)\n",
    "    MEAN_RET_Dense = pl.evauate_strategy(t, outputs_Dense, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_Dense, ACC_Dense, MEAN_RET_Dense])\n",
    "    \n",
    "result_price_df['Dense'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['Dense'] = np.std(res, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):   \n",
    "    model_LSTM = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM.to(device)\n",
    "    model_LSTM\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM, targets, MSE = pl.evaluate(model_LSTM, test_x, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_LSTM = pp.min_max_norm_inverse(outputs_LSTM.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_LSTM = np.mean((outputs_LSTM-t)**2)\n",
    "    ACC_LSTM = pl.direction_accuracy(outputs_LSTM, t)\n",
    "    MEAN_RET_LSTM = pl.evauate_strategy(t, outputs_LSTM, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_LSTM, ACC_LSTM, MEAN_RET_LSTM])\n",
    "    \n",
    "result_price_df['LSTM'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['LSTM'] = np.std(res, axis = 0)\n",
    "print(result_price_df['LSTM'])\n",
    "print(result_price_df_std['LSTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):   \n",
    "    model_GRU = ml.GRU_base(\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=32,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU.to(device)\n",
    "    model_GRU\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    chunksize= 1\n",
    "    plt.plot(pl.smooth_loss(train_loss, chunksize = chunksize) , label=\"train loss\")\n",
    "    plt.plot(pl.smooth_loss(val_loss, chunksize = chunksize) , label=\"val loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show() \n",
    "\n",
    "    outputs_GRU, targets, MSE = pl.evaluate(model_GRU, test_x, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_GRU = pp.min_max_norm_inverse(outputs_GRU.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_GRU = np.mean((outputs_GRU-t)**2)\n",
    "    ACC_GRU = pl.direction_accuracy(outputs_GRU, t)\n",
    "    MEAN_RET_GRU = pl.evauate_strategy(t, outputs_GRU, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_GRU, ACC_GRU, MEAN_RET_GRU])\n",
    "    \n",
    "result_price_df['GRU'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['GRU'] = np.std(res, axis = 0)\n",
    "print(result_price_df['GRU'])\n",
    "print(result_price_df_std['GRU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Add additional features for patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_aug.copy()\n",
    "test_x = test_x_aug.copy()\n",
    "val_x = val_x_aug.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, val_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):    \n",
    "    model_Dense = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = nn,\n",
    "        input_size = train_x.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense.to(device)\n",
    "    model_Dense\n",
    "\n",
    "    lr= 0.001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense,\n",
    "        train_x_ = train_x.reshape(train_x.shape[0],-1),\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x.reshape(val_x.shape[0],-1),\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "\n",
    "    outputs_Dense, targets, MSE = pl.evaluate(model_Dense, test_x.reshape(test_x.shape[0],-1), test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_Dense = pp.min_max_norm_inverse(outputs_Dense.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_Dense = np.mean((outputs_Dense-t)**2)\n",
    "    ACC_Dense = pl.direction_accuracy(outputs_Dense, t)\n",
    "    MEAN_RET_Dense = pl.evauate_strategy(t, outputs_Dense, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_Dense, ACC_Dense, MEAN_RET_Dense])\n",
    "    \n",
    "result_price_df['Dense_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['Dense_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_price_df['Dense_aug_feat'])\n",
    "print(result_price_df_std['Dense_aug_feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):    \n",
    "    model_LSTM = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM.to(device)\n",
    "    model_LSTM\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM, targets, MSE = pl.evaluate(model_LSTM, test_x, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_LSTM = pp.min_max_norm_inverse(outputs_LSTM.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_LSTM = np.mean((outputs_LSTM-t)**2)\n",
    "    ACC_LSTM = pl.direction_accuracy(outputs_LSTM, t)\n",
    "    MEAN_RET_LSTM = pl.evauate_strategy(t, outputs_LSTM, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_LSTM,ACC_LSTM,MEAN_RET_LSTM])\n",
    "    \n",
    "result_price_df['LSTM_aug_feat_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['LSTM_aug_feat_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_price_df['LSTM_aug_feat_aug_feat'])\n",
    "print(result_price_df_std['LSTM_aug_feat_aug_feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU = ml.GRU_base(\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=32,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU.to(device)\n",
    "    model_GRU\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_GRU, targets, MSE = pl.evaluate(model_GRU, test_x, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_GRU = pp.min_max_norm_inverse(outputs_GRU.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_GRU = np.mean((outputs_GRU-t)**2)\n",
    "    ACC_GRU = pl.direction_accuracy(outputs_GRU, t)\n",
    "    MEAN_RET_GRU = pl.evauate_strategy(t, outputs_GRU, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_GRU, ACC_GRU, MEAN_RET_GRU])\n",
    "    \n",
    "result_price_df['GRU_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['GRU_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_price_df['GRU_aug_feat'])\n",
    "print(result_price_df_std['GRU_aug_feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Add Auto-Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dense + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense_AE = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = nn,\n",
    "        input_size = train_x_smooth.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense_AE.to(device)\n",
    "    model_Dense_AE\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense_AE,\n",
    "        train_x_ = train_x_smooth.reshape(train_x_smooth.shape[0],-1),\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth.reshape(val_x_smooth.shape[0],-1),\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_Dense_AE, targets, MSE = pl.evaluate(model_Dense_AE, test_x_smooth.reshape(test_x_smooth.shape[0],-1), test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_Dense_AE = pp.min_max_norm_inverse(outputs_Dense_AE.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_Dense_AE = np.mean((outputs_Dense_AE-t)**2)\n",
    "    ACC_Dense_AE = pl.direction_accuracy(outputs_Dense_AE, t)\n",
    "    MEAN_RET_Dense_AE = pl.evauate_strategy(t, outputs_Dense_AE, plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_Dense_AE, ACC_Dense_AE, MEAN_RET_Dense_AE])\n",
    "    \n",
    "result_price_df['Dense_AE'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['Dense_AE'] = np.std(res, axis = 0)\n",
    "print(result_price_df['Dense_AE'])\n",
    "print(result_price_df_std['Dense_AE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LSTM + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_LSTM_AE = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.1,\n",
    "            device=device)\n",
    "    model_LSTM_AE.to(device)\n",
    "    model_LSTM_AE\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM_AE,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM_AE, targets, MSE = pl.evaluate(model_LSTM_AE, test_x_smooth, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_LSTM_AE = pp.min_max_norm_inverse(outputs_LSTM_AE.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_LSTM_AE = np.mean((outputs_LSTM_AE-t)**2)\n",
    "    ACC_LSTM_AE = pl.direction_accuracy(outputs_LSTM_AE, t)\n",
    "    MEAN_RET_LSTM_AE = pl.evauate_strategy(t, outputs_LSTM_AE, plot=True, tax= tax_)\n",
    "\n",
    "    \n",
    "    res.append([MSE_LSTM_AE, ACC_LSTM_AE, MEAN_RET_LSTM_AE])\n",
    "    \n",
    "result_price_df['LSTM_AE'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['LSTM_AE'] = np.std(res, axis = 0)\n",
    "print(result_price_df['LSTM_AE'])\n",
    "print(result_price_df_std['LSTM_AE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### GRU + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU_AE = ml.GRU_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU_AE.to(device)\n",
    "    model_GRU_AE\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU_AE,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_GRU_AE, targets, MSE = pl.evaluate(model_GRU_AE, test_x_smooth, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_GRU_AE = pp.min_max_norm_inverse(outputs_GRU_AE.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_GRU_AE = np.mean((outputs_GRU_AE-t)**2)\n",
    "    ACC_GRU_AE = pl.direction_accuracy(outputs_GRU_AE, t)\n",
    "    MEAN_RET_GRU_AE = pl.evauate_strategy(t, outputs_GRU_AE, plot=True, tax=tax_)\n",
    "\n",
    "    res.append([MSE_GRU_AE, ACC_GRU_AE, MEAN_RET_GRU_AE])\n",
    "    \n",
    "result_price_df['GRU_AE'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['GRU_AE'] = np.std(res, axis = 0)\n",
    "print(result_price_df['GRU_AE'])\n",
    "print(result_price_df_std['GRU_AE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Add Regularized Loss: $L = \\sum (y- \\hat{y})^2 + \\lambda \\sum ((y- y_{prev})(y- \\hat{y}))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dense + AE + REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense_AE_REG = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = nn,\n",
    "        input_size = train_x_smooth.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense_AE_REG.to(device)\n",
    "    model_Dense_AE_REG\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train_regularized(\n",
    "        model=model_Dense_AE_REG,\n",
    "        train_x_ = train_x_smooth.reshape(train_x_smooth.shape[0],-1),\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth.reshape(val_x_smooth.shape[0],-1),\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lambda_=0.01,\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_Dense_AE_REG, targets, MSE = pl.evaluate(model_Dense_AE_REG, test_x_smooth.reshape(test_x_smooth.shape[0],-1), test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_Dense_AE_REG = pp.min_max_norm_inverse(outputs_Dense_AE_REG.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_Dense_AE_REG = np.mean((outputs_Dense_AE_REG-t)**2)\n",
    "    ACC_Dense_AE_REG = pl.direction_accuracy(outputs_Dense_AE_REG, t)\n",
    "    MEAN_RET_Dense_AE_REG = pl.evauate_strategy(t, outputs_Dense_AE_REG,plot=True, tax = tax_)\n",
    "    \n",
    "    res.append([MSE_Dense_AE_REG, ACC_Dense_AE_REG, MEAN_RET_Dense_AE_REG])\n",
    "    \n",
    "result_price_df['Dense_AE_REG'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['Dense_AE_REG'] = np.std(res, axis = 0)\n",
    "print(result_price_df['Dense_AE_REG'])\n",
    "print(result_price_df_std['Dense_AE_REG'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LSTM + AE + REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_LSTM_AE_REG = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM_AE_REG.to(device)\n",
    "    model_LSTM_AE_REG\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train_regularized(\n",
    "        model=model_LSTM_AE_REG,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        lambda_=0.01,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM_AE_REG, targets, MSE = pl.evaluate(model_LSTM_AE_REG, test_x_smooth, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_LSTM_AE_REG = pp.min_max_norm_inverse(outputs_LSTM_AE_REG.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_LSTM_AE_REG = np.mean((outputs_LSTM_AE_REG-t)**2)\n",
    "    ACC_LSTM_AE_REG = pl.direction_accuracy(outputs_LSTM_AE_REG, t)\n",
    "    MEAN_RET_LSTM_AE_REG = pl.evauate_strategy(t, outputs_LSTM_AE_REG, plot=True, tax=tax_)\n",
    "\n",
    "    res.append([MSE_LSTM_AE_REG, ACC_LSTM_AE_REG, MEAN_RET_LSTM_AE_REG])\n",
    "    \n",
    "result_price_df['LSTM_AE_REG'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['LSTM_AE_REG'] = np.std(res, axis = 0)\n",
    "print(result_price_df['LSTM_AE_REG'])\n",
    "print(result_price_df_std['LSTM_AE_REG'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### GRU + AE + REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU_AE_REG = ml.GRU_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU_AE_REG.to(device)\n",
    "    model_GRU_AE_REG\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train_regularized(\n",
    "        model=model_GRU_AE_REG,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y_reg[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y_reg[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        lambda_=0.01,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_GRU_AE_REG, targets, MSE = pl.evaluate(model_GRU_AE_REG, test_x_smooth, test_y_reg[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    outputs_GRU_AE_REG = pp.min_max_norm_inverse(outputs_GRU_AE_REG.reshape(-1,), tuple_min_max_=tuple_min_max) \n",
    "    t = pp.min_max_norm_inverse(targets.reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MSE_GRU_AE_REG = np.mean((outputs_GRU_AE_REG-t)**2)\n",
    "    ACC_GRU_AE_REG = pl.direction_accuracy(outputs_GRU_AE_REG, t)\n",
    "    MEAN_RET_GRU_AE_REG = pl.evauate_strategy(t, outputs_GRU_AE_REG, plot=True, tax = tax_)\n",
    "    \n",
    "    \n",
    "    res.append([MSE_GRU_AE_REG, ACC_GRU_AE_REG, MEAN_RET_GRU_AE_REG])\n",
    "    \n",
    "result_price_df['GRU_AE_REG'] = np.mean(res, axis = 0)\n",
    "result_price_df_std['GRU_AE_REG'] = np.std(res, axis = 0)\n",
    "print(result_price_df['GRU_AE_REG'])\n",
    "print(result_price_df_std['GRU_AE_REG'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trend prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice to extract original features\n",
    "train_x = train_x_aug.copy()\n",
    "test_x = test_x_aug.copy()\n",
    "val_x = val_x_aug.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_x_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_n = 32\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense_Trend = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = n_n,\n",
    "        input_size = train_x.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense_Trend.to(device)\n",
    "    model_Dense_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense_Trend,\n",
    "        train_x_ = train_x.reshape(train_x.shape[0],-1),\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x.reshape(val_x.shape[0],-1),\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_Dense_Trend, targets, ACC_Dense_Trend = pl.evaluate_trend(model_Dense_Trend, test_x.reshape(test_x.shape[0],-1), test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_Dense_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0], tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_Dense_Trend = pl.evauate_strategy_trend(t, outputs_Dense_Trend,plot=True, tax =tax_)\n",
    "\n",
    "    \n",
    "    res.append([ACC_Dense_Trend, MEAN_RET_Dense_Trend])\n",
    "    \n",
    "result_trend_df['Dense'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['Dense'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['Dense'])\n",
    "print(result_trend_df_std['Dense'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_LSTM_Trend = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM_Trend.to(device)\n",
    "    model_LSTM_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM_Trend,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM_Trend, targets, ACC_LSTM_Trend = pl.evaluate_trend(model_LSTM_Trend, test_x, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_LSTM_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_LSTM_Trend = pl.evauate_strategy_trend(t, outputs_LSTM_Trend,plot=True, tax=tax_)\n",
    "    \n",
    "    res.append([ACC_LSTM_Trend, MEAN_RET_LSTM_Trend])\n",
    "    \n",
    "result_trend_df['LSTM'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['LSTM'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['LSTM'])\n",
    "print(result_trend_df_std['LSTM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU_Trend = ml.GRU_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU_Trend.to(device)\n",
    "    model_GRU_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU_Trend,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_GRU_Trend, targets, ACC_GRU_Trend = pl.evaluate_trend(model_GRU_Trend, test_x, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_GRU_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_GRU_Trend = pl.evauate_strategy_trend(t, outputs_GRU_Trend, plot=True, tax=tax_)\n",
    "    \n",
    "    res.append([ACC_GRU_Trend, MEAN_RET_GRU_Trend])\n",
    "    \n",
    "result_trend_df['GRU'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['GRU'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['GRU'])\n",
    "print(result_trend_df_std['GRU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Add augmented features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_aug.copy()\n",
    "test_x = test_x_aug.copy()\n",
    "val_x = val_x_aug.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, val_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense_Trend = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = n_n,\n",
    "        input_size = train_x.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense_Trend.to(device)\n",
    "    model_Dense_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense_Trend,\n",
    "        train_x_ = train_x.reshape(train_x.shape[0],-1),\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x.reshape(val_x.shape[0],-1),\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_Dense_Trend, targets, ACC_Dense_Trend = pl.evaluate_trend(model_Dense_Trend, test_x.reshape(test_x.shape[0],-1), test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_Dense_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0], tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_Dense_Trend = pl.evauate_strategy_trend(t, outputs_Dense_Trend,plot=True, tax=tax_)\n",
    "    \n",
    "    res.append([ACC_Dense_Trend, MEAN_RET_Dense_Trend])\n",
    "    \n",
    "result_trend_df['Dense_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['Dense_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['Dense_aug_feat'])\n",
    "print(result_trend_df_std['Dense_aug_feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_LSTM_Trend = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM_Trend.to(device)\n",
    "    model_LSTM_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM_Trend,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_LSTM_Trend, targets, ACC_LSTM_Trend = pl.evaluate_trend(model_LSTM_Trend, test_x, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_LSTM_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_LSTM_Trend = pl.evauate_strategy_trend(t, outputs_LSTM_Trend,plot=True, tax=tax_)\n",
    "\n",
    "    res.append([ACC_LSTM_Trend, MEAN_RET_LSTM_Trend])\n",
    "    \n",
    "result_trend_df['LSTM_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['LSTM_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['LSTM_aug_feat'])\n",
    "print(result_trend_df_std['LSTM_aug_feat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU_Trend = ml.GRU_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU_Trend.to(device)\n",
    "    model_GRU_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU_Trend,\n",
    "        train_x_ = train_x,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_GRU_Trend, targets, ACC_GRU_Trend = pl.evaluate_trend(model_GRU_Trend, test_x, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_GRU_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_GRU_Trend = pl.evauate_strategy_trend(t, outputs_GRU_Trend, plot=True, tax=tax_)\n",
    "\n",
    "    res.append([ACC_GRU_Trend, MEAN_RET_GRU_Trend])\n",
    "    \n",
    "result_trend_df['GRU_aug_feat'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['GRU_aug_feat'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['GRU_aug_feat'])\n",
    "print(result_trend_df_std['GRU_aug_feat'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Add Auto-Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Dense + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_Dense_AE_Trend = ml.Dense(\n",
    "        num_layers = nl,\n",
    "        layer_size = n_n,\n",
    "        input_size = train_x_smooth.reshape(train_x.shape[0],-1).shape[1],\n",
    "        output_size = 1,\n",
    "        dropout=0.05)\n",
    "    model_Dense_AE_Trend.to(device)\n",
    "    model_Dense_AE_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_Dense_AE_Trend,\n",
    "        train_x_ = train_x_smooth.reshape(train_x.shape[0],-1),\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth.reshape(val_x.shape[0],-1),\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        lr_=lr,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_Dense_AE_Trend, targets, ACC_Dense_AE_Trend = pl.evaluate_trend(model_Dense_AE_Trend, test_x_smooth.reshape(test_x.shape[0],-1), test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_Dense_AE_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0], tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_Dense_AE_Trend = pl.evauate_strategy_trend(t, outputs_Dense_AE_Trend,plot=True, tax=tax_)\n",
    "\n",
    "    res.append([ACC_Dense_AE_Trend, MEAN_RET_Dense_AE_Trend])\n",
    "    \n",
    "result_trend_df['Dense_AE'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['Dense_AE'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['Dense_AE'])\n",
    "print(result_trend_df_std['Dense_AE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LSTM + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_LSTM_AE_Trend = ml.LSTM_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_LSTM_AE_Trend.to(device)\n",
    "    model_LSTM_AE_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_LSTM_AE_Trend,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "    outputs_LSTM_AE_Trend, targets, ACC_LSTM_AE_Trend = pl.evaluate_trend(model_LSTM_AE_Trend, test_x_smooth, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_LSTM_AE_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_LSTM_AE_Trend = pl.evauate_strategy_trend(t, outputs_LSTM_AE_Trend,plot=True, tax=tax_)\n",
    "    \n",
    "    res.append([ACC_LSTM_AE_Trend, MEAN_RET_LSTM_AE_Trend])\n",
    "    \n",
    "result_trend_df['LSTM_AE'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['LSTM_AE'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['LSTM_AE'])\n",
    "print(result_trend_df_std['LSTM_AE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### GRU + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(10):\n",
    "    model_GRU_AE_Trend = ml.GRU_base(\n",
    "            hidden_size=16,\n",
    "            num_layers=2,\n",
    "            input_size= train_x_smooth.shape[2],\n",
    "            out_features_lin=16,\n",
    "            out_features_end=1,\n",
    "            dropout=0.05,\n",
    "            device=device)\n",
    "    model_GRU_AE_Trend.to(device)\n",
    "    model_GRU_AE_Trend\n",
    "\n",
    "    lr= 0.0001\n",
    "    num_epochs= 1000\n",
    "\n",
    "    train_loss, val_loss = pl.train(\n",
    "        model=model_GRU_AE_Trend,\n",
    "        train_x_ = train_x_smooth,\n",
    "        train_y_ =  train_y[:,0].reshape(-1,1),\n",
    "        val_x_ = val_x_smooth,\n",
    "        val_y_=  val_y[:,0].reshape(-1, 1),\n",
    "        batch_size_=bs,\n",
    "        num_epochs_=num_epochs,\n",
    "        criterion_ = nn.BCEWithLogitsLoss(pos_weight=positive_weight),\n",
    "        lr_=lr,\n",
    "        device_=device,\n",
    "        verbose = 1)\n",
    "\n",
    "\n",
    "    outputs_GRU_AE_Trend, targets, ACC_GRU_AE_Trend = pl.evaluate_trend(model_GRU_AE_Trend, test_x_smooth, test_y[:,0].reshape(-1,1), device=device) \n",
    "\n",
    "    sns.heatmap(data=confusion_matrix(targets, outputs_GRU_AE_Trend), annot=True)\n",
    "    plt.plot()\n",
    "\n",
    "    tuple_min_max= norm[2]\n",
    "    t = pp.min_max_norm_inverse(test_y_reg[:,0].reshape(-1,), tuple_min_max_=tuple_min_max)\n",
    "\n",
    "    MEAN_RET_GRU_AE_Trend = pl.evauate_strategy_trend(t, outputs_GRU_AE_Trend, plot=True, tax=tax_)\n",
    "    \n",
    "    res.append([ACC_GRU_AE_Trend, MEAN_RET_GRU_AE_Trend])\n",
    "    \n",
    "result_trend_df['GRU_AE'] = np.mean(res, axis = 0)\n",
    "result_trend_df_std['GRU_AE'] = np.std(res, axis = 0)\n",
    "print(result_trend_df['GRU_AE'])\n",
    "print(result_trend_df_std['GRU_AE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_price_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_price_df_std.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_trend_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_trend_df_std.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6cd1e2431ed720d58c3d7433ac38193d2b2409498dcfdacb5e7d8bbbdf24f0b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
